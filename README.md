# ğŸ“ Seattle 911 Call Data Pipeline (Batch + Stream)

## ğŸŒŸ Features

| Batch                             | Stream                         |
| --------------------------------- | ------------------------------ |
| âœ… CSV to Parquet/Snowflake       | âœ… Real-time API ingestion     |
| âœ… Star schema generation         | âœ… Kafka stream processing     |
| âœ… Historical data transformation | âœ… Cassandra storage           |
| âœ… Data quality checks            | âœ… Micro-batch transformations |

---

## ğŸ—ï¸ System Architecture

![Pipeline Architecture](Images/System_Architecture.png)

---

## ğŸ“‚ Directory Structure

```bash
ETL_Pipeline/
â”œâ”€â”€ Batch/                    # [Batch] Historical processing
â”‚   â”œâ”€â”€ extract.py            # CSV ingestion
â”‚   â”œâ”€â”€ transform.py          # Star schema
â”‚   â””â”€â”€ load.py               # Snowflake/Parquet
â”‚
â”œâ”€â”€ Stream/                   # [Stream] Real-time
â”‚   â”œâ”€â”€ producer.py           # Kafka producer
â”‚   â”œâ”€â”€ consumer.py           # Sparkâ†’Cassandra
â”‚   â””â”€â”€ cassandra_setup/      # DB config
â”‚
â”œâ”€â”€ Data/                     # Inputs
â””â”€â”€ Parquet_files/            # Batch outputs
```

---

## ğŸ§Š Batch Processing

### ğŸ”§ Technical Stack

- PySpark 3.5+
- Snowflake
- Parquet

### ğŸš€ Quick Start

```bash
python ETL_Pipeline_Batch/main.py
```

### âš™ï¸ Pipeline Stages

| Stage     | Description                 |
| --------- | --------------------------- |
| Extract   | Reads CSV from `Data/`      |
| Transform | Creates star schema         |
| Load      | Writes to Parquet/Snowflake |

---

## ğŸŒŸ Star Schema Diagram

```mermaid
erDiagram
    fact_call {
        bigint dim_cad_event_id
        bigint dim_location_id
        bigint dim_call_sign_id
        bigint dim_care_spd_id
        bigint dim_co_response_id
        int response_time_s
        int service_time_s
    }
    fact_call }|--|| dim_cad_event : "cad_event"
    fact_call }|--|| dim_location : "location"
    fact_call }|--|| dim_call_sign : "call_sign"
    fact_call }|--|| dim_care_spd : "care_spd"
    fact_call }|--|| dim_co_response : "co_response"
```

---

## âš¡ Stream Processing

### ğŸ”§ Technical Stack

- PySpark Streaming 3.5+
- Kafka
- Cassandra

### ğŸš€ Quick Start

```bash
# Start infrastructure
docker-compose up -d

# Run pipeline
python Stream/producer.py &
python Stream/consumer.py
```

### âš™ï¸ Components

| Component | Description                 |
| --------- | --------------------------- |
| Producer  | Fetches API â†’ Kafka topic   |
| Consumer  | Spark Streaming â†’ Cassandra |
| Cassandra | Stores in `police_calls`    |

---

### ğŸ“Š Cassandra Schema

```sql
CREATE TABLE police_calls (
    cad_event_number text PRIMARY KEY,
    call_type text,
    processed_at timestamp
    -- ...45+ fields
);
```

---

### ğŸ” Sample Query

```sql
SELECT call_type, COUNT(*)
FROM seattle_data.police_calls
GROUP BY call_type;
```

## ğŸ“Š Power BI Dashboards

### Overview Dashboard

<img src="Images/power_bi_overview.png" alt="Power BI Overview Dashboard" width="1000" style="max-width:100%;">

### Key Metrics Dashboard

<img src="Images/power_bi_meausres.png" alt="Power BI Measures Dashboard" width="1000" style="max-width:100%;">

---

## ğŸ“ License

Apache 2.0 â€” See [`LICENSE`](./LICENSE) for details
